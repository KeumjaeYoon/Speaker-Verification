{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88c59401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, bottleneck=128):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(bottleneck), # I remove this layer\n",
    "            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.se(input)\n",
    "        return input * x\n",
    "\n",
    "class Bottle2neck(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale = 8):\n",
    "        super(Bottle2neck, self).__init__()\n",
    "        width       = int(math.floor(planes / scale))\n",
    "        self.conv1  = nn.Conv1d(inplanes, width*scale, kernel_size=1)\n",
    "        self.bn1    = nn.BatchNorm1d(width*scale)\n",
    "        self.nums   = scale -1\n",
    "        convs       = []\n",
    "        bns         = []\n",
    "        num_pad = math.floor(kernel_size/2)*dilation\n",
    "        for i in range(self.nums):\n",
    "            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))\n",
    "            bns.append(nn.BatchNorm1d(width))\n",
    "        self.convs  = nn.ModuleList(convs)\n",
    "        self.bns    = nn.ModuleList(bns)\n",
    "        self.conv3  = nn.Conv1d(width*scale, planes, kernel_size=1)\n",
    "        self.bn3    = nn.BatchNorm1d(planes)\n",
    "        self.relu   = nn.ReLU()\n",
    "        self.width  = width\n",
    "        self.se     = SEModule(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        spx = torch.split(out, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i==0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.relu(sp)\n",
    "            sp = self.bns[i](sp)\n",
    "            if i==0:\n",
    "                out = sp\n",
    "            else:\n",
    "                out = torch.cat((out, sp), 1)\n",
    "        out = torch.cat((out, spx[self.nums]),1)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out = self.se(out)\n",
    "        out += residual\n",
    "        return out \n",
    "\n",
    "class PreEmphasis(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, coef: float = 0.97):\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        self.register_buffer(\n",
    "            'flipped_filter', torch.FloatTensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.tensor) -> torch.tensor:\n",
    "        input = input.unsqueeze(1)\n",
    "        input = F.pad(input, (1, 0), 'reflect')\n",
    "        return F.conv1d(input, self.flipped_filter).squeeze(1)\n",
    "\n",
    "class FbankAug(nn.Module):\n",
    "\n",
    "    def __init__(self, freq_mask_width = (0, 8), time_mask_width = (0, 10)):\n",
    "        self.time_mask_width = time_mask_width\n",
    "        self.freq_mask_width = freq_mask_width\n",
    "        super().__init__()\n",
    "\n",
    "    def mask_along_axis(self, x, dim):\n",
    "        original_size = x.shape\n",
    "        batch, fea, time = x.shape\n",
    "        if dim == 1:\n",
    "            D = fea\n",
    "            width_range = self.freq_mask_width\n",
    "        else:\n",
    "            D = time\n",
    "            width_range = self.time_mask_width\n",
    "\n",
    "        mask_len = torch.randint(width_range[0], width_range[1], (batch, 1), device=x.device).unsqueeze(2)\n",
    "        mask_pos = torch.randint(0, max(1, D - mask_len.max()), (batch, 1), device=x.device).unsqueeze(2)\n",
    "        arange = torch.arange(D, device=x.device).view(1, 1, -1)\n",
    "        mask = (mask_pos <= arange) * (arange < (mask_pos + mask_len))\n",
    "        mask = mask.any(dim=1)\n",
    "\n",
    "        if dim == 1:\n",
    "            mask = mask.unsqueeze(2)\n",
    "        else:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        x = x.masked_fill_(mask, 0.0)\n",
    "        return x.view(*original_size)\n",
    "\n",
    "    def forward(self, x):    \n",
    "        x = self.mask_along_axis(x, dim=2)\n",
    "        x = self.mask_along_axis(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class ECAPA_TDNN(nn.Module):\n",
    "\n",
    "    def __init__(self, C):\n",
    "\n",
    "        super(ECAPA_TDNN, self).__init__()\n",
    "\n",
    "        self.torchfbank = torch.nn.Sequential(\n",
    "            PreEmphasis(),            \n",
    "            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=400, hop_length=160, \\\n",
    "                                                 f_min = 20, f_max = 7600, window_fn=torch.hamming_window, n_mels=80),\n",
    "            )\n",
    "\n",
    "        self.specaug = FbankAug() # Spec augmentation\n",
    "\n",
    "        self.conv1  = nn.Conv1d(80, C, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu   = nn.ReLU()\n",
    "        self.bn1    = nn.BatchNorm1d(C)\n",
    "        self.layer1 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=8)\n",
    "        self.layer2 = Bottle2neck(C, C, kernel_size=3, dilation=3, scale=8)\n",
    "        self.layer3 = Bottle2neck(C, C, kernel_size=3, dilation=4, scale=8)\n",
    "        self.layer4 = nn.Conv1d(3*C, 1536, kernel_size=1)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(4608, 256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Tanh(), # I add this layer\n",
    "            nn.Conv1d(256, 1536, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "            )\n",
    "        self.bn5 = nn.BatchNorm1d(3072)\n",
    "        self.fc6 = nn.Linear(3072, 192)\n",
    "        self.bn6 = nn.BatchNorm1d(192)\n",
    "\n",
    "\n",
    "    def forward(self, x, aug=True):\n",
    "        with torch.no_grad():\n",
    "            x = self.torchfbank(x)+1e-6\n",
    "            x = x.log()   \n",
    "            x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            if aug == True:\n",
    "                x = self.specaug(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x+x1)\n",
    "        x3 = self.layer3(x+x1+x2)\n",
    "                \n",
    "        x = self.layer4(torch.cat((x1,x2,x3),dim=1))\n",
    "        x = self.relu(x)\n",
    "\n",
    "        t = x.size()[-1]\n",
    "\n",
    "        global_x = torch.cat((x,torch.mean(x,dim=2,keepdim=True).repeat(1,1,t), torch.sqrt(torch.var(x,dim=2,keepdim=True).clamp(min=1e-4)).repeat(1,1,t)), dim=1)\n",
    "        \n",
    "        w = self.attention(global_x)\n",
    "\n",
    "        mu = torch.sum(x * w, dim=2)\n",
    "        sg = torch.sqrt((torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-4) )\n",
    "\n",
    "        x = torch.cat((mu,sg),1)\n",
    "        x = self.bn5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.bn6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a5bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=8):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, channel // reduction),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(channel // reduction, channel),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ResNetSE(nn.Module):\n",
    "    def __init__(self, block, layers, num_filters, nOut, encoder_type='SAP', n_mels=40, log_input=True, **kwargs):\n",
    "        super(ResNetSE, self).__init__()\n",
    "        \n",
    "        self.inplanes   = num_filters[0]\n",
    "        self.encoder_type = encoder_type\n",
    "        \n",
    "        self.torchfbank = torch.nn.Sequential(\n",
    "            PreEmphasis(),            \n",
    "            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=400, hop_length=160, \\\n",
    "                                                 f_min = 20, f_max = 7600, window_fn=torch.hamming_window, n_mels=80),\n",
    "            )\n",
    "\n",
    "        self.specaug = FbankAug() # Spec augmentation\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, num_filters[0] , kernel_size=7, stride=(2, 1), padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, num_filters[0], layers[0])\n",
    "        self.layer2 = self._make_layer(block, num_filters[1], layers[1], stride=(2, 2))\n",
    "        self.layer3 = self._make_layer(block, num_filters[2], layers[2], stride=(2, 2))\n",
    "        self.layer4 = self._make_layer(block, num_filters[3], layers[3], stride=(1, 1))\n",
    "\n",
    "        if self.encoder_type == \"SAP\":\n",
    "            self.sap_linear = nn.Linear(num_filters[3] * block.expansion, num_filters[3] * block.expansion)\n",
    "            self.attention = self.new_parameter(num_filters[3] * block.expansion, 1)\n",
    "            out_dim = num_filters[3] * block.expansion\n",
    "        elif self.encoder_type == \"ASP\":\n",
    "            self.sap_linear = nn.Linear(num_filters[3] * block.expansion, num_filters[3] * block.expansion)\n",
    "            self.attention = self.new_parameter(num_filters[3] * block.expansion, 1)\n",
    "            out_dim = num_filters[3] * block.expansion * 2\n",
    "        else:\n",
    "            raise ValueError('Undefined encoder')\n",
    "\n",
    "        self.fc = nn.Linear(out_dim, nOut)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def new_parameter(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, aug=True):\n",
    "        with torch.no_grad():\n",
    "            x = self.torchfbank(x)+1e-6\n",
    "            x = x.log()   \n",
    "            x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            if aug == True:\n",
    "                x = self.specaug(x)\n",
    "            x = x.unsqueeze(dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2, keepdim=True)\n",
    "\n",
    "        if self.encoder_type == \"SAP\":\n",
    "            x = x.permute(0,3,1,2).squeeze(-1)\n",
    "            h = torch.tanh(self.sap_linear(x))\n",
    "            w = torch.matmul(h, self.attention).squeeze(dim=2)\n",
    "            w = F.softmax(w, dim=1).view(x.size(0), x.size(1), 1)\n",
    "            x = torch.sum(x * w, dim=1)\n",
    "        elif self.encoder_type == \"ASP\":\n",
    "            x = x.permute(0,3,1,2).squeeze(-1)\n",
    "            h = torch.tanh(self.sap_linear(x))\n",
    "            w = torch.matmul(h, self.attention).squeeze(dim=2)\n",
    "            w = F.softmax(w, dim=1).view(x.size(0), x.size(1), 1)\n",
    "            mu = torch.sum(x * w, dim=1)\n",
    "            rh = torch.sqrt( ( torch.sum((x**2) * w, dim=1) - mu**2 ).clamp(min=1e-5) )\n",
    "            x = torch.cat((mu,rh),1)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def MainModel(nOut=192, **kwargs):\n",
    "    # Number of filters\n",
    "    num_filters = [16, 32, 64, 128]\n",
    "    model = ResNetSE(SEBasicBlock, [3, 4, 6, 3], num_filters, nOut, **kwargs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
