{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd24ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tools.ipynb\n",
      "importing Jupyter notebook from loss.ipynb\n",
      "importing Jupyter notebook from model.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, os, tqdm, numpy, soundfile, time, pickle, random, import_ipynb\n",
    "import torch.nn as nn\n",
    "from tools import *\n",
    "from loss import *\n",
    "from model import *\n",
    "\n",
    "class ECAPAModel(nn.Module):\n",
    "    def __init__(self, lr, lr_decay, C , n_class, m, s, test_step, loss_name, encoder, **kwargs):\n",
    "        super(ECAPAModel, self).__init__()\n",
    "        ## model\n",
    "        speaker_encoder = {'ecapa' : ECAPA_TDNN(C = C).cuda(),\n",
    "                               'resnetse' : MainModel().cuda()}\n",
    "        \n",
    "        self.speaker_encoder = speaker_encoder[encoder]\n",
    "\n",
    "        ## Classifier\n",
    "        loss = {'softmax' : CrossEntropyLoss(n_class = n_class, m = m, s = s).cuda(),\n",
    "       'amsoftmax' : AMSoftmax(n_class = n_class, m = m, s = s).cuda(),\n",
    "       'aamsoftmax' : AAMsoftmax(n_class = n_class, m = m, s = s).cuda(),\n",
    "       'elastic_am' : ElasticLoss_am(n_class = n_class, m = m, s = s).cuda(),\n",
    "       'elastic_aam' : ElasticLoss_aam(n_class = n_class, m = m, s = s).cuda(),            \n",
    "               }\n",
    "        self.speaker_loss    = loss[loss_name]     \n",
    "\n",
    "        self.optim           = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = 2e-5)\n",
    "        self.scheduler       = torch.optim.lr_scheduler.StepLR(self.optim, step_size = test_step, gamma=lr_decay)\n",
    "        print(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number = %.2f\"%(sum(param.numel() for param in self.speaker_encoder.parameters()) / 1024 / 1024))\n",
    "\n",
    "    def train_network(self, epoch, loader):\n",
    "        self.train()\n",
    "        ## Update the learning rate based on the current epcoh\n",
    "        self.scheduler.step(epoch - 1)\n",
    "        index, top1, loss = 0, 0, 0\n",
    "        lr = self.optim.param_groups[0]['lr']\n",
    "        for num, (data, labels) in enumerate(loader, start = 1):\n",
    "            self.zero_grad()\n",
    "            labels            = torch.LongTensor(labels).cuda()\n",
    "            speaker_embedding = self.speaker_encoder.forward(data.cuda(), aug = True)\n",
    "            nloss, prec       = self.speaker_loss.forward(speaker_embedding, labels)\n",
    "    #\t\t\tnloss, prec       = self.speaker_loss.forward(epoch, speaker_embedding, labels)  # uniform by epoch\n",
    "            nloss.backward()\n",
    "            self.optim.step()\n",
    "            index += len(labels)\n",
    "            top1 += prec\n",
    "            loss += nloss.detach().cpu().numpy()\n",
    "            sys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") + \\\n",
    "            \" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n",
    "            \" Loss: %.5f, ACC: %2.2f%% \\r\"        %(loss/(num), top1/index*len(labels)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        return loss/num, lr, top1/index*len(labels)\n",
    "\n",
    "    def eval_network(self, eval_list, eval_path, eval_frame=300):\n",
    "        self.eval()\n",
    "        files = []\n",
    "        embeddings = {}\n",
    "        lines = open(eval_list).read().splitlines()\n",
    "        for line in lines:\n",
    "            files.append(line.split()[1])\n",
    "            files.append(line.split()[2])\n",
    "        setfiles = list(set(files))\n",
    "        setfiles.sort()\n",
    "\n",
    "        for idx, file in tqdm.tqdm(enumerate(setfiles), total = len(setfiles)):\n",
    "            audio, _  = soundfile.read(os.path.join(eval_path, file))\n",
    "            # Full utterance\n",
    "#             data_1 = torch.FloatTensor(numpy.stack([audio],axis=0)).cuda()\n",
    "\n",
    "            # Spliited utterance matrix\n",
    "            max_audio = eval_frame * 160 + 240\n",
    "            if audio.shape[0] <= max_audio:\n",
    "                shortage = max_audio - audio.shape[0]\n",
    "                audio = numpy.pad(audio, (0, shortage), 'wrap')\n",
    "            # 1s,2s,3s\n",
    "#             start = int(random.random()*(audio.shape[0]-max_audio))\n",
    "#             audio = audio[start:start+max_audio]\n",
    "            data_1 = torch.FloatTensor(numpy.stack([audio],axis=0)).cuda() \n",
    "                \n",
    "            feats = []          \n",
    "            startframe = numpy.linspace(0, audio.shape[0]-max_audio, num=5)\n",
    "            for asf in startframe:\n",
    "                feats.append(audio[int(asf):int(asf)+max_audio])\n",
    "            feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n",
    "            data_2 = torch.FloatTensor(feats).cuda()\n",
    "            # Speaker embeddings\n",
    "            with torch.no_grad():\n",
    "                embedding_1 = self.speaker_encoder.forward(data_1, aug = False)\n",
    "                embedding_1 = F.normalize(embedding_1, p=2, dim=1)\n",
    "                embedding_2 = self.speaker_encoder.forward(data_2, aug = False)\n",
    "                embedding_2 = F.normalize(embedding_2, p=2, dim=1)\n",
    "            embeddings[file] = [embedding_1, embedding_2]\n",
    "        scores, labels  = [], []\n",
    "\n",
    "        for line in lines:\n",
    "            embedding_11 = embeddings[line.split()[1]] , embedding_12 = embeddings[line.split()[1]]\n",
    "            embedding_21 = embeddings[line.split()[2]] , embedding_22 = embeddings[line.split()[2]]\n",
    "            # Compute the scores\n",
    "            score_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T)) # higher is positive\n",
    "            score_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n",
    "            score = (score_1 + score_2) / 2\n",
    "            score = score_1.detach().cpu().numpy()\n",
    "            scores.append(score)\n",
    "            labels.append(int(line.split()[0]))\n",
    "            \n",
    "        # Coumpute EER and minDCF\n",
    "        EER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n",
    "        fnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n",
    "        minDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1)\n",
    "        return EER, minDCF\n",
    "\n",
    "\n",
    "    \n",
    "    def save_parameters(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_parameters(self, path):\n",
    "        self_state = self.state_dict()\n",
    "        loaded_state = torch.load(path)\n",
    "        for name, param in loaded_state.items():\n",
    "            origname = name\n",
    "            if name not in self_state:\n",
    "                name = name.replace(\"module.\", \"\")\n",
    "                if name not in self_state:\n",
    "                    print(\"%s is not in the model.\"%origname)\n",
    "                    continue\n",
    "            if self_state[name].size() != loaded_state[origname].size():\n",
    "                print(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "                continue\n",
    "            self_state[name].copy_(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
